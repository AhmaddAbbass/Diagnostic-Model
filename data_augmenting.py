# -*- coding: utf-8 -*-
"""Untitled15.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_mnMUZg7nYxm5lroMQww0GCtDZVlob7I
"""

!pip install nltk

# Mount your Google Drive first
from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import random
import re
import nltk
from nltk.corpus import wordnet
from nltk.tokenize import word_tokenize
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans

# Download required NLTK data if you haven't already
nltk.download('wordnet')
nltk.download('omw-1.4')
nltk.download('punkt')
nltk.download('punkt_tab')

def synonym_replacement(text, replacement_prob=0.3):
    """
    Replace words in the text with their synonyms randomly with a given probability.
    """
    words = word_tokenize(text)
    new_words = []
    for word in words:
        if word.isalpha() and random.random() < replacement_prob:
            synsets = wordnet.synsets(word)
            if synsets:
                lemmas = synsets[0].lemma_names()
                lemmas = [lemma.replace('_', ' ') for lemma in lemmas if lemma.lower() != word.lower()]
                if lemmas:
                    new_word = random.choice(lemmas)
                    new_words.append(new_word)
                else:
                    new_words.append(word)
            else:
                new_words.append(word)
        else:
            new_words.append(word)
    new_text = " ".join(new_words)
    return new_text

def augment_text(text, num_augmented=3, replacement_prob=0.3):
    """
    Generate a list of augmented text samples based on the original text.
    """
    augmented_texts = []
    for _ in range(num_augmented):
        new_text = synonym_replacement(text, replacement_prob=replacement_prob)
        augmented_texts.append(new_text)
    return augmented_texts

def cluster_and_generate(csv_path, output_csv, num_clusters=3, aug_multiplier=3):
    """
    Loads dataset, clusters the text samples using TF-IDF and k-means, then generates
    synthetic samples for each cluster via text augmentation.

    The augmented dataset (including both original and synthetic entries) is saved as a CSV.

    Parameters:
    - csv_path: Full path to the original CSV file (with "label" and "text" columns).
    - output_csv: Full path to where the output CSV file will be saved.
    - num_clusters: Number of clusters to use in k-means.
    - aug_multiplier: Number of synthetic samples to generate for each original sample.
    """
    # Load dataset
    df = pd.read_csv(csv_path)

    # Vectorize text using TF-IDF
    texts = df['Text'].tolist()
    vectorizer = TfidfVectorizer(stop_words='english')
    X = vectorizer.fit_transform(texts)

    # Perform k-means clustering
    kmeans = KMeans(n_clusters=num_clusters, random_state=42)
    clusters = kmeans.fit_predict(X)
    df['cluster'] = clusters

    # Print cluster distribution for reference
    cluster_counts = df['cluster'].value_counts().to_dict()
    print("Cluster distribution:", cluster_counts)

    synthetic_data = []

    # Generate synthetic samples for each cluster
    for cluster in sorted(cluster_counts.keys()):
        cluster_samples = df[df['cluster'] == cluster]
        for index, row in cluster_samples.iterrows():
            original_text = row['Text']
            synthetic_variants = augment_text(original_text, num_augmented=aug_multiplier)
            for variant in synthetic_variants:
                synthetic_data.append({
                    "Disease name": row['Disease name'],
                    "Text": variant,
                    "cluster": cluster
                })

    synthetic_df = pd.DataFrame(synthetic_data)
    combined_df = pd.concat([df, synthetic_df], ignore_index=True)
    combined_df.to_csv(output_csv, index=False)
    print(f"Synthetic data (original + augmented) saved to {output_csv}")

# Example usage:
# Update these paths to point to the location of your dataset in Google Drive.
input_csv = "/content/drive/MyDrive/EECE_490/Project/datasets_texts/Vitiligo/Vitiligo.csv"
output_csv = "/content/drive/MyDrive/EECE_490/Project/datasets_texts/Vitiligo/Vitiligo_augmented.csv"

# Run the clustering and augmentation function
cluster_and_generate(csv_path=input_csv, output_csv=output_csv, num_clusters=3, aug_multiplier=3)