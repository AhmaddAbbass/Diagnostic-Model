# fusion/cross_attention.py
"""
cross_attention.py
------------------
(Section “Fusion Strategies” – Late)
Two‑stream cross‑attention transformer blocks over image (v) and text (w),
followed by pooling + MLP.  
State‑of‑the‑art robust fusion handling missing T.
"""
